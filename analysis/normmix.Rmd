---
title: "Illustration of EM and IP solutions to mixture optimization problem"
author: "Peter Carbonetto"
date: August 23, 2017
output:
  html_document:
    theme: readable
    include:
      before_body: include/header.html
      after_body: include/footer.html
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  comment   = "#",
  results   = "hold",
  collapse  = TRUE,
  fig.align = "center")
```

## Introduction

Here, we compare three algorithms for solving what I call the "mixture
distribution optimization problem." This problem can be motivated from
many different applications (e..g,
[Bovy *et al*](http://dx.doi.org/10.1214/10-AOAS439) and
[this vignette from the REBayes package](https://cran.r-project.org/web/packages/REBayes/vignettes/rebayes.pdf)),
but can be defined very concisely as follows:

$$
\begin{array}{ll}
\mbox{minimize}   &\displaystyle  -\sum_{i=1}^n \log \sum_{j=1}^k L_{ij} w_j \\
\mbox{subject to} & w_j \geq 0, \mbox{for $j = 1, \ldots, k$} \\
                  & w_1 + \cdots + w_k = 1
\end{array}
$$

This is a convex optimization problem in most circumstances. And, in
cases where the $L_{ij}$'s can be interpreted as conditional
likelihoods under some probabilistic model, we can compute the
solution using a very simple EM algorithm. (In this case, the $w_j$'s
can be interpreted as the mixture weights for a mixture model.)
However, as we will see below, this simple EM algorithm is a terrible
solution, even for very simple models.

(*Note:* we might want to generalize this problem formulation in the
future to allow for priors on the mixture weights and/or weighted
samples.)

Here, we compare the EM algorithm against a primal-dual interior-point
method applied to the dual formulation of this optimization
problem. We consider the simple situation in which we would like to
fit a mixture model in which the likelihood is univariate normal,
possibly with heterogeneous variances, and the prior is a mixture of
univariate normals. Even this very simple case is useful for
illustrating the pros & cons of the algorithms.

Although there is already an excellent interior-point solution
implemented in the
[REBayes package](https://cran.r-project.org/package=REBayes), which
uses [MOSEK](https://mosek.com) to solve the dual problem (note:
[IPOPT](https://projects.coin-or.org/Ipopt) also has an R interface),
here we implement our own primal-dual interior-point method because it
is useful to get insight into the computation involved. (By contrast,
it is hard to get much insight into the numerical computation involved
when taking the
[disciplined convex programming](http://dcp.stanford.edu) approach in
which the solvers use symbolic or automatic differentiation, e.g.,
MOSEK). Also, eventually it will be useful to optimize the computation
to accommodate large-scale situations with very large data sets and/or
large mixtures.

In this experiment, we use the IP method to solve both the primal
optimization problem as well as the dual formulation. (See the REBayes
package vignette mentioned above for the formulation of the dual.)
Solving the dual problem probably isn't a good idea because the number
of optimization variables in the dual problem is on the order of the
number of samples. However, it is a bit simpler to implement because
it does not have any equality constraints. As we will see, the IP
method applied to the primal problem is much faster for data sets with
large numbers of samples.

There are other reasons why an interior-point method may not be a good
solution---one important reason is that it doesn't easily allow for an
initial solution that is on the boundary of the feasible set (e.g.,
mixture weights that are exactly zero). Nonetheless, it is instructive
to explore the convergence properties of the interior-point method for
this problem.

## Analysis setup

I begin by loading a few packages, as well as some additional
functions I wrote for this analysis.

```{r load-pkgs, message=FALSE}
library(Matrix)
library(ggplot2)
library(cowplot)
source("../code/misc.R")
source("../code/datasim.R")
source("../code/likelihood.R")
source("../code/ipsolver.R")
source("../code/mixopt.R")
```

These variables determine how the data set is generated: the random
number generator seed, the number of data samples (n), the standard
errors of the samples (se), and the standard deviations (s) and
mixture weights (w) used to simulate the data.

```{r data}
seed <- 1
n    <- 5000
se   <- rep(0.1,n)
sim  <- list(s = c(0,   0.1, 0.2, 0.5),
             w = c(0.95,0.03,0.01,0.01))
```

Also, note that heterogeneous standard errors (se) are allowed.

Next, I specify the model parameters---specifically, the standard
deviations of the normal mixture components.

```{r model}
s <- c(0.01,10^(seq(-2,0,length.out = 19)))
```

## Generate data set

Simulate a data set with n samples.

```{r sim-data}
cat(sprintf("Simulating data set with %d observations.\n",n))
set.seed(1)
k <- length(s)
x <- datasim.norm(sim$w,sim$s,se)
```

## Compute likelihood matrix

Compute the n x k conditional likelihood matrix.

```{r, calc-likelihood}
cat(sprintf("Computing the %d x %d conditional likelihood matrix.\n",n,k))
L <- condlikmatrix.norm(x,se,s)
```

## Fit mixture model using EM

First, fit the mixture model using the EM algorithm. This is a very
simple algorithm---after computing the conditional likelihood matrix,
the E and M steps can be implemented in only a few lines of code:

```{r, eval=FALSE}
# E Step
P <- t(t(L) * w)
P <- P / (rowSums(P) + eps)

# M step
w <- colMeans(P)
```

Here, eps is a small number near zero (e.g., 1e-8).

Observe that individual EM iterations are fast but it takes many
iterations to converge to a solution.

```{r, em}
out <- system.time(fit.em <- mixopt.em(L,tol = 1e-4,verbose = FALSE))
cat(sprintf("Model fitting took %d iterations and %0.2f seconds.\n",
            length(fit.em$maxd),out["elapsed"]))
```

## Fit mixture model using IP solver---primal formulation

The primal-dual interior-point solver is based on the algorithm
described by [Armand *et al*](https://doi.org/10.1137/S1052623498344720).
It is substantially more complicated than the EM algorithm, and
individual iterations are more expensive, but it takes only a small
number of iterations to converge to a solution.

```{r, ip}
out <- system.time(fit.ip <- mixopt.ip(L))
cat(sprintf("Model fitting took %d iterations and %0.2f seconds.\n",
            length(fit.ip$maxd),out["elapsed"]))
```

## Fit mixture model using IP solver---dual formulation

Dual optimization problem

```{r, dualip}
out <- system.time(fit.dualip <- mixopt.dualip(L))
cat(sprintf("Model fitting took %d iterations and %0.2f seconds.\n",
            length(fit.dualip$maxd),out["elapsed"]))
```

The EM algorithm implements a very simple convergence criterion---the
maximum difference between the iterates must be small---whereas the
convergence in the IP method is based on how close we are to
satisfying the KKT optimality conditions.

*Note:* if you have the RMosek and REBayes packages installed, you can
compare the output of the IP method to running

```{r rebayes, echo=TRUE, eval=FALSE}
REBayes:KWDual(L,rep(1,k),rep(1,n))
```

## Plots showing improvement in EM & IP solutions over time

This first plot shows the maximum change in the solution against the
running time of the EM algorithm.

```{r plot-delta-vs-time-em}
plot.delta.vs.time <- function (timing, maxd, color, plot.title) {
  m  <- length(maxd)
  i  <- 2:(m-1)
  return(ggplot(data.frame(time = timing[i,"elapsed"],
                           maxd = maxd[i]),
                aes(x = time,y = maxd)) +
         geom_line(col = color,size = 0.5) +
         geom_point(col = color,shape = 20) +
         scale_y_continuous(breaks = 10^seq(-4,2),trans = "log10") +
         labs(x     = "elapsed time (seconds)",
              y     = "max. change in solution",
              title = plot.title))
}
p1 <- plot.delta.vs.time(fit.em$timing,fit.em$maxd,"darkorange","EM algorithm")
```

The next two plots also show the maximum change in the solution at
each iteration, but for the IP solvers.

```{r plot-delta-vs-time-ip}
p2 <- plot.delta.vs.time(fit.ip$timing,fit.ip$maxd,"royalblue",
                         "IP algorithm (primal)")
p3 <- plot.delta.vs.time(fit.dualip$timing,fit.dualip$ipsolver$maxd,
						 "darkblue","IP algorithm (dual)")
```

This plot shows the distance of the (primal) objective function to the
minimum against the running time of the EM algorithm. (Here, we take
the "minimum" to be the best solution among the three algorithms.)

```{r plot-objective-vs-time-em}
best.sol <- min(c(fit.em$obj,fit.ip$obj,fit.dualip$obj))
plot.obj.vs.time <- function (timing, obj, min.obj, color, plot.title) {
  m  <- length(obj)
  i  <- 2:(m-1)
  return(ggplot(data.frame(time = timing[i,"elapsed"],
                           y    = obj[i] - min.obj),
                aes(x = time,y = y)) +
  geom_line(col = color,size = 0.5) +
  geom_point(col = color,shape = 20) +
  scale_y_continuous(breaks = 10^(-6:3),trans = "log10") +
  labs(x     = "elapsed time (seconds)",
       y     = "dist. from min.",
       title = plot.title))
}
p4 <- plot.obj.vs.time(fit.em$timing,fit.em$obj,best.sol,
                       "darkorange","EM algorithm")
```

These two plots show the value of the objective against the running
time of the IP solver. For the dual formulation, the distance to the
dual objective is shown.

```{r plot-objective-vs-time-ip}
p5 <- plot.obj.vs.time(fit.ip$timing,fit.ip$obj,best.sol,"royalblue",
                       "IP algorithm (primal)")
p6 <- plot.obj.vs.time(fit.dualip$timing,fit.dualip$ipsolver$obj,
                       min(fit.dualip$ipsolver$obj),"darkblue",
                       "IP algorithm (dual)")
```
Arrange all six plots in a single figure.

```{r plot-grid, fig.width=7, fig.height=7}
adjust.plot <- function (p)
  p + theme_cowplot(font_size = 12) +
    theme(plot.title = element_text(face = "plain"))
print(plot_grid(adjust.plot(p1),adjust.plot(p4),
                adjust.plot(p2),adjust.plot(p5),
                adjust.plot(p3),adjust.plot(p6),
				nrow = 3))
```

Points in the plots indicate individual iterations.

I plotted the results in this way to show the *improvement in the
solution over time.* (With the caveat that the dual formulation of the
IP method shows the improvement in the dual objective.) In this way,
we can clearly see the differences in how the EM and IP methods
behave:

+ The IP method moves rapidly toward the solution, but each iteration
  is more expensive.

+ EM algorithm never quite reaches the solution, even after a large
  number of iterations; the panels in the top row both highlight the
  poor convergence properties of EM. 

+ We also see that the dual IP method is slower to converge than the
  primal IP, and the per-iteration cost is substantialy higher.

## Session information

This is the version of R and the packages that were used to generate
these results.

```{r session-info}
sessionInfo()
```

---
title: "Illustration of EM and IP solutions to mixture optimization problem"
author: "Peter Carbonetto"
date: August 9, 2017
output:
  html_document:
    theme: readable
    include:
      before_body: include/header.html
      after_body: include/footer.html
---

```{r knitr, echo=FALSE}
knitr::opts_chunk$set(
  comment   = "#",
  results   = "hold",
  collapse  = TRUE,
  fig.align = "center",
  fig.path  = paste0("../docs/figure/",knitr::current_input(),"/"))
```

## Introduction

Here, we compare two algorithms for solving what I call the "mixture
distribution optimization problem." This problem can be motivated from
many different applications (e..g,
[Bovy *et al*](http://dx.doi.org/10.1214/10-AOAS439) and
[this vignette from the REBayes package](https://cran.r-project.org/web/packages/REBayes/vignettes/rebayes.pdf)),
but can be defined very concisely as follows:

$$
\begin{array}{ll}
\mbox{minimize}   &\displaystyle  -\sum_{i=1}^n \log \sum_{j=1}^k L_{ij} w_j \\
\mbox{subject to} & w_j \geq 0, \mbox{for $j = 1, \ldots, k$} \\
                  & w_1 + \cdots + w_k = 1
\end{array}
$$

This is a convex optimization problem in most circumstances. And, in
cases where the $L_{ij}$'s can be interpreted as conditional
likelihoods under some probabilistic model, we can compute the
solution using a very simple EM algorithm. (In this case, the $w_j$'s
can be interpreted as the mixture weights for a mixture model.)
However, as we will see below, this simple EM algorithm is a terrible
solution, even for very simple models.

(*Note:* we might want to generalize this problem formulation in the
future to allow for priors on the mixture weights and/or weighted
samples.)

Here, we compare the EM algorithm against a primal-dual interior-point
method applied to the dual formulation of this optimization
problem. We consider the simple situation in which we would like to
fit a mixture model in which the likelihood is univariate normal,
possibly with heterogeneous variances, and the prior is a mixture of
univariate normals. Even this very simple case is useful for
illustrating the pros & cons of the algorithms.

Although there is already an excellent interior-point solution
implemented in the
[REBayes package](https://cran.r-project.org/package=REBayes), which
uses [MOSEK](https://mosek.com) to solve the dual problem (note:
[IPOPT](https://projects.coin-or.org/Ipopt) also has an R interface),
here we implement our own primal-dual interior-point method because it
is useful to get insight into the computation involved. (By contrast,
it is hard to get much insight into the numerical computation involved
when taking the
[disciplined convex programming](http://dcp.stanford.edu) approach in
which the solvers use symbolic or automatic differentiation, e.g.,
MOSEK). Also, eventually it will be useful to optimize the computation
to accommodate large-scale situations with very large data sets and/or
large mixtures.

Solving the dual problem probably isn't a good idea because the number
of optimization variables in the dual problem is on the order of the
number of samples (see the REBayes package vignette mentioned above),
but it is a bit simpler to implement because it does not have any
equality constraints. An "action item" is to implement the primal-dual
interior-point method for the primal problem, which will have a much
more manageable computational complexity.

There are other reasons why an interior-point method may not be a good
solution---one important reason is that it doesn't easily allow for an
initial solution that is on the boundary of the feasible set (e.g.,
mixture weights that are exactly zero). Nonetheless, it is instructive
to explore the convergence properties of the interior-point method for
this problem.

## Analysis setup

I begin by loading a few packages, as well as some additional
functions I wrote for this analysis.

```{r load-pkgs, message=FALSE}
library(Matrix)
library(ggplot2)
library(cowplot)
source("../code/datasim.R")
source("../code/likelihood.R")
source("../code/ipsolver.R")
source("../code/mixopt.R")
```

These variables determine how the data set is generated: the random
number generator seed, the number of data samples (n), the standard
errors of the samples (se), and the standard deviations (s) and
mixture weights (w) used to simulate the data.

```{r data}
seed <- 1
n    <- 500
se   <- rep(0.1,n)
sim  <- list(s = c(0,   0.1, 0.2, 0.5),
             w = c(0.95,0.03,0.01,0.01))
```

*Note:* do not make the sample size (n) too large otherwise the
interior-point method will be very slow. (As I mentioned above.)

Also, note that heterogeneous standard errors (se) are allowed.

Next, I specify the model parameters---specifically, the standard
deviations of the normal mixture components.

```{r model}
s <- c(0.01,10^(seq(-2,0,length.out = 39)))
```

## Generate data set

Simulate a data set with n samples.

```{r sim-data}
cat(sprintf("Simulating data set with %d observations.\n",n))
set.seed(1)
k <- length(s)
x <- datasim.norm(sim$w,sim$s,se)
```

## Compute likelihood matrix

Compute the n x k conditional likelihood matrix.

```{r, calc-likelihood}
cat(sprintf("Computing the %d x %d conditional likelihood matrix.\n",n,k))
L <- condlikmatrix.norm(x,se,s)
```

## Fit mixture model using EM

First, fit the mixture model using the EM algorithm. This is a very
simple algorithm---after computing the conditional likelihood matrix,
the E and M steps can be implemented in only a few lines of code:

```{r, eval=FALSE}
# E Step
P <- t(t(L) * w)
P <- P / (rowSums(P) + eps)

# M step
w <- colMeans(P)
```

Here, eps is a small number near zero (e.g., 1e-8).

Observe that individual EM iterations are fast but it takes many
iterations to converge to a solution.

```{r, em}
out <- system.time(fit.em <- mixopt.em(L,tol = 1e-4,verbose = FALSE))
cat(sprintf("Model fitting took %d iterations and %0.2f seconds.\n",
            length(fit.em$maxd),out["elapsed"]))
```

## Fit mixture model using IP solver

The primal-dual interior-point solver is based on the algorithm
described by [Armand *et al*](https://doi.org/10.1137/S1052623498344720).
It is substantially more complicated than the EM algorithm, and
individual iterations are more expensive, but it takes only a small
number of iterations to converge to a solution.

```{r, ip}
out <- system.time(fit.ip <- mixopt.dualip(L))
cat(sprintf("Model fitting took %d iterations and %0.2f seconds.\n",
            length(fit.ip$maxd),out["elapsed"]))
```

The EM algorithm implements a very simple convergence criterion---the
maximum difference between the iterates must be small---whereas the
convergence in the IP method is based on how close we are to
satisfying the KKT optimality conditions.

*Note:* if you have the RMosek and REBayes packages installed, you can
compare the output of the IP method to running

```{r rebayes, echo=TRUE, eval=FALSE}
REBayes:KWDual(L,rep(1,k),rep(1,n))
```

## Plots showing improvement in EM & IP solutions over time

This first plot shows the (maximum) change in the mixture weights
against the running time of the EM algorithm.

```{r plot-delta-vs-time-em}
m  <- length(fit.em$maxd)
i  <- 2:(m-1)
p1 <- ggplot(data.frame(time = fit.em$timing[i,"elapsed"],
                        maxd = fit.em$maxd[i]),
             aes(x = time,y = maxd)) +
  geom_line(col = "darkorange",size = 0.5) +
  geom_point(col = "darkorange",shape = 20) +
  scale_y_continuous(breaks = 10^seq(-4,-1),trans = "log10") +
  labs(x     = "elapsed time (seconds)",
       y     = "max. change in solution",
       title = "EM algorithm")
```

The second plot is the same thing, but for the IP solver.

```{r plot-delta-vs-time-ip}
m  <- length(fit.ip$maxd)
i  <- 2:(m-1)
p2 <- ggplot(data.frame(time = fit.ip$timing[i,"elapsed"],
                        maxd = fit.ip$maxd[i]),
             aes(x = time,y = maxd)) +
  geom_line(col = "darkblue",size = 0.5) +
  geom_point(col = "darkblue",shape = 20) +
  scale_y_continuous(breaks = 10^(-3:1),trans = "log10") +
  labs(x     = "elapsed time (seconds)",
       y     = "max. change in solution",
       title = "IP algorithm")
```

This next plot shows the distance of the (primal) objective function
to the minimum against running time of the EM algorithm. (Here, we
take the ground-truth minimizer to be the IP solution because it is
much better than the EM solution.)

```{r}
m  <- length(fit.em$obj)
i  <- 2:(m-1)
p3 <- ggplot(data.frame(time = fit.em$timing[i,"elapsed"],
                        y    = fit.em$obj[i] - fit.ip$obj),
                        aes(x = time,y = y)) +
  geom_line(col = "darkorange",size = 0.5) +
  geom_point(col = "darkorange",shape = 20) +
  scale_y_continuous(breaks = 10^(-1:1),trans = "log10") +
  labs(x     = "elapsed time (seconds)",
       y     = "distance from primal min.",
       title = "EM algorithm")
```

The fourth and final plot show the value of the dual objective
function against running time of the IP solver.

```{r}
m  <- length(fit.ip$ipsolver$obj)
i  <- 2:(m-1)
p4 <- ggplot(data.frame(time = fit.ip$timing[i,"elapsed"],
                        y = fit.ip$ipsolver$obj[i] - min(fit.ip$ipsolver$obj)),
                        aes(x = time,y = y)) +
  geom_line(col = "darkorange",size = 0.5) +
  geom_point(col = "darkorange",shape = 20) +
  scale_y_continuous(breaks = 10^seq(-8,2,2),trans = "log10") +
  labs(x     = "elapsed time (seconds)",
       y     = "distance from dual min.",
       title = "IP algorithm")
```

Arrange all four plots in a single figure.

```{r plot-grid, fig.width=8, fig.height=6}
adjust.plot <- function (p)
  p + theme_cowplot(font_size = 12) +
    theme(plot.title = element_text(face = "plain"))
print(plot_grid(adjust.plot(p1),adjust.plot(p3),
                adjust.plot(p2),adjust.plot(p4)))
```

Points in the plots indicate individual iterations.

I plotted the results in this way to show the *improvement in the
solution over time.* In this way, we can clearly see the differences
in how the EM and IP methods behave:

+ The IP method moves quickly toward the solution, but each iteration
  is more expensive.

+ The EM algorithm never quite reaches the solution; the panels in the
  top row both highlight the poor convergence properties of EM. By
  contrast, the IP method exhibits much more rapid convergence to the
  solution.

## Session information

This is the version of R and the packages that were used to generate
these results.

```{r session-info}
sessionInfo()
```
